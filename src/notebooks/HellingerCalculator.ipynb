{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much of code is credit to: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "%matplotlib inline\n",
    "from nltk.metrics.spearman import *\n",
    "from nltk.metrics import ContingencyMeasures\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.00082696e-01 7.33392555e-08 1.00000000e+00 6.00082696e-01\n",
      "  6.00082576e-01 6.00082636e-01 6.00082696e-01 6.00082726e-01\n",
      "  6.00082636e-01 9.99999928e-01]\n",
      " [1.46678511e-07 6.00082787e-01 6.00082546e-01 1.46678511e-07\n",
      "  2.07434740e-07 1.46678511e-07 6.55966243e-08 1.85535271e-07\n",
      "  1.60678258e-07 6.00082666e-01]\n",
      " [1.60678258e-07 6.00082696e-01 6.00082576e-01 1.60678258e-07\n",
      "  1.46678511e-07 9.27676357e-08 9.27676357e-08 2.17559390e-07\n",
      "  1.46678511e-07 6.00082726e-01]\n",
      " [1.13616686e-07 6.00082696e-01 6.00082636e-01 1.13616686e-07\n",
      "  1.31193249e-07 6.55966243e-08 6.55966243e-08 1.60678258e-07\n",
      "  9.27676357e-08 6.00082696e-01]\n",
      " [6.00082606e-01 1.00000000e+00 7.33392555e-08 6.00082606e-01\n",
      "  6.00082636e-01 6.00082606e-01 6.00082516e-01 6.00082636e-01\n",
      "  6.00082636e-01 9.99999856e-01]\n",
      " [1.46678511e-07 6.00082787e-01 6.00082546e-01 1.46678511e-07\n",
      "  2.07434740e-07 1.46678511e-07 6.55966243e-08 1.85535271e-07\n",
      "  1.60678258e-07 6.00082666e-01]\n",
      " [1.46678511e-07 6.00082636e-01 6.00082696e-01 1.46678511e-07\n",
      "  9.27676357e-08 6.55966243e-08 1.46678511e-07 1.85535271e-07\n",
      "  9.27676357e-08 6.00082666e-01]\n",
      " [6.00082576e-01 9.99999928e-01 1.00000000e+00 6.00082576e-01\n",
      "  6.00082606e-01 6.00082636e-01 6.00082636e-01 6.00082516e-01\n",
      "  6.00082606e-01 9.27676357e-08]\n",
      " [1.85535271e-07 6.00082606e-01 6.00082636e-01 1.85535271e-07\n",
      "  1.13616686e-07 9.27676357e-08 1.60678258e-07 2.36511992e-07\n",
      "  1.46678511e-07 6.00082726e-01]\n",
      " [2.17559390e-07 6.00082576e-01 6.00082756e-01 2.17559390e-07\n",
      "  1.31193249e-07 1.46678511e-07 2.36511992e-07 2.45440094e-07\n",
      "  1.60678258e-07 6.00082666e-01]]\n",
      "[[list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]\n",
      " [list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n",
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  list([[], ['wait_winner', 'defenses_diver', 'quicken_reality', 'mix_move', 'college_condition', 'going_hand']])]]\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "#gets a specified index and returns text data from dataframe \n",
    "def iterator(index):\n",
    "    \n",
    "    labels = ['ID','Name','Date','topicName','scrubbedtext']\n",
    "    podKnow_Data = pd.DataFrame.from_records(results, columns = labels)\n",
    "    \n",
    "    #isolate scrubbed text values and convert to lowercase to avoid duplicates\n",
    "    scrubbedData = str(podKnow_Data.iloc[index-1:index, 4].values).lower()\n",
    "    \n",
    "    #remove junk values\n",
    "    scrubbedData = scrubbedData.replace(\"\\\"\", \"\").replace(\",\", \"\").replace(\"\\'\",  \"\").splitlines()\n",
    "            \n",
    "    return scrubbedData\n",
    "\n",
    "#lemmitizes words\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\"\".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "#tokenizes words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "\n",
    "#loads two models and computes hellinger distance betweeen them     \n",
    "def computeHellingerDistance():\n",
    "                \n",
    "            m1 = LdaMulticore.load(\"model1\")\n",
    "            m2 = LdaMulticore.load(\"model2\")\n",
    "\n",
    "            mdiff, annotation =  m1.diff(m2, distance='hellinger', annotation = True)\n",
    "            topic_diff = mdiff\n",
    "            \n",
    "            #prints hellinger distance between topics \n",
    "            print(topic_diff)\n",
    "            \n",
    "            #prints commonalities in words between a given topic and the topic it's being compared to \n",
    "            print(annotation)\n",
    "    \n",
    "        \n",
    "\n",
    "#creates and formats data and saves models for later comparison \n",
    "def formatDataAndModel(finalBigrams, counter):\n",
    "    \n",
    "            #remove junk values \n",
    "            data_words_bigrams = finalBigrams.replace(\"(\", \"\").replace(\")\", \"\").replace(\"'\", \" \").replace(\",\" ,\"\")\n",
    "\n",
    "            \n",
    "            data_words_bigrams = data_words_bigrams.split()\n",
    "            \n",
    "            finalBigrams = (list(sorted((data_words_bigrams))))\n",
    "            \n",
    "            #separate bigrams by '_' character and combine them into a single value \n",
    "            finalBigrams = [i+ '_' + j for i,j in zip(finalBigrams[::2], finalBigrams[1::2])]\n",
    "            \n",
    "            data_lemmatized = lemmatization(finalBigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "            #maps IDs to words\n",
    "            id2word = corpora.Dictionary(data_lemmatized)\n",
    "    \n",
    "            #simply receives lemmitized text\n",
    "            texts = data_lemmatized\n",
    "    \n",
    "            #maps new lemmitized data to IDs\n",
    "            corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    \n",
    "            #generates LDA model from input data \n",
    "            lda_model = LdaMulticore(\n",
    "            corpus=corpus, num_topics=10, id2word=id2word,\n",
    "            workers=4, eval_every=None, passes=10, batch=True)\n",
    "        \n",
    "            #saves LDA model with counter index \n",
    "            lda_model.save(r\"model\" + str(counter))\n",
    "            \n",
    "            if(counter == 2):\n",
    "                computeHellingerDistance()\n",
    "        \n",
    "            \n",
    "                                        \n",
    "\n",
    "results = []\n",
    "counter = 0\n",
    "totalList = []\n",
    "\n",
    "\n",
    "#driver code block that ideally computes distance between the only two transcripts in a folder\n",
    "saveLocation = r'C:\\Users\\Frank Einstein\\Podknow\\data\\transcripts\\gcsst\\scrubbed'\n",
    "for folderName,subfolders,fileName in os.walk(r'C:\\Users\\Frank Einstein\\Podknow\\data\\transcripts\\gcsst\\scrubbed'):\n",
    "    \n",
    "    \n",
    "    try:   \n",
    "          for file in fileName:\n",
    "            if str(file.endswith(\".txt_scrubbed\")):\n",
    "                f = open(os.path.join(folderName,file),'rb')\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "            value0, value1, value2,value3, *extraWords = file.split('_')\n",
    "            value4 = data\n",
    "            rows = (value0,value1,value2,value3, value4)\n",
    "            results.append(rows)\n",
    "            \n",
    "            finalBigrams = \"\"\n",
    "            \n",
    "            counter = counter + 1\n",
    "            \n",
    "            #gets scrubbed data from a given counter index\n",
    "            data = iterator(counter)\n",
    "            \n",
    "            #tokenizes data                                                \n",
    "            tokens = nltk.wordpunct_tokenize(str(data))\n",
    "            \n",
    "            nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "            \n",
    "            #this block removes junk characters and only accepts words 3 letters or longer\n",
    "            finder = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "            finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "            \n",
    "            #find top 200 best bigrams    \n",
    "            bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "            bigrams = list(sorted(finder.nbest(bigram_measures.likelihood_ratio, 20)))\n",
    "            \n",
    "            #convert bigrams to string for formatting\n",
    "            for x in bigrams: \n",
    "                finalBigrams += str(x)\n",
    "            \n",
    "\n",
    "            formatDataAndModel(finalBigrams, counter)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "    except:\n",
    "        print(\"error\")\n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
