{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook computes hellinger distance between two saved LDA models, with results closer to 0 being more closely related and results closer to 1 meaning there is a greater distance between topics. Topic 1 for a given model is compared with every other topic in the next model, then topic 2 is compared with every other topic in the next model, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The notebook is currently configured to compute hellinger distance for bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much of code is credit to: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "%matplotlib inline\n",
    "from nltk.metrics.spearman import *\n",
    "from nltk.metrics import ContingencyMeasures\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets a specified index and returns text data from dataframe \n",
    "def iterator(index):\n",
    "    \n",
    "    labels = ['ID','Name','Date','topicName','scrubbedtext']\n",
    "    podKnow_Data = pd.DataFrame.from_records(results, columns = labels)\n",
    "    \n",
    "    #isolate scrubbed text values and convert to lowercase to avoid duplicates\n",
    "    scrubbedData = str(podKnow_Data.iloc[index-1:index, 4].values).lower()\n",
    "    \n",
    "    #remove junk values\n",
    "    scrubbedData = scrubbedData.replace(\"\\\"\", \"\").replace(\",\", \"\").replace(\"\\'\",  \"\").splitlines()\n",
    "            \n",
    "    return scrubbedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmitizes words\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\"\".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizes words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads two models and computes hellinger distance betweeen them     \n",
    "def computeHellingerDistance():\n",
    "                \n",
    "            m1 = LdaMulticore.load(\"model1\")\n",
    "            m2 = LdaMulticore.load(\"model2\")\n",
    "\n",
    "            mdiff, annotation =  m1.diff(m2, distance='hellinger', annotation = True)\n",
    "            topic_diff = mdiff\n",
    "            \n",
    "            #prints hellinger distance between topics \n",
    "            print(topic_diff)\n",
    "            \n",
    "            #prints commonalities in words between a given topic and the topic it's being compared to \n",
    "            print(annotation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates and formats data and saves models for later comparison \n",
    "def formatDataAndModel(finalBigrams, counter):\n",
    "    \n",
    "            #remove junk values \n",
    "            data_words_bigrams = finalBigrams.replace(\"(\", \"\").replace(\")\", \"\").replace(\"'\", \" \").replace(\",\" ,\"\")\n",
    "\n",
    "           \n",
    "            data_words_bigrams = data_words_bigrams.split()\n",
    "            \n",
    "            finalBigrams = (list(sorted((data_words_bigrams))))\n",
    "            \n",
    "            #separate bigrams by '_' character and combine them into a single value \n",
    "            finalBigrams = [i+ '_' + j for i,j in zip(finalBigrams[::2], finalBigrams[1::2])]\n",
    "            \n",
    "            data_lemmatized = lemmatization(finalBigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "            #maps IDs to words\n",
    "            id2word = corpora.Dictionary(data_lemmatized)\n",
    "    \n",
    "            #simply receives lemmitized text\n",
    "            texts = data_lemmatized\n",
    "    \n",
    "            #maps new lemmitized data to IDs\n",
    "            corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    \n",
    "            #generates LDA model from input data \n",
    "            lda_model = LdaMulticore(\n",
    "            corpus=corpus, num_topics=10, id2word=id2word,\n",
    "            workers=4, eval_every=None, passes=10, batch=True)\n",
    "        \n",
    "            #saves LDA model with counter index \n",
    "            lda_model.save(r\"model\" + str(counter))\n",
    "            \n",
    "            if(counter == 2):\n",
    "                computeHellingerDistance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "counter = 0\n",
    "totalList = []\n",
    "\n",
    "\n",
    "#driver code block that ideally computes distance between the only two transcripts in a folder\n",
    "saveLocation = r'C:\\Users\\Frank Einstein\\Podknow\\data\\transcripts\\gcsst\\scrubbed'\n",
    "for folderName,subfolders,fileName in os.walk(r'C:\\Users\\Frank Einstein\\Podknow\\data\\transcripts\\gcsst\\scrubbed'):\n",
    "    \n",
    "    \n",
    "    try:   \n",
    "          for file in fileName:\n",
    "            if str(file.endswith(\".txt_scrubbed\")):\n",
    "                f = open(os.path.join(folderName,file),'rb')\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "            value0, value1, value2,value3, *extraWords = file.split('_')\n",
    "            value4 = data\n",
    "            rows = (value0,value1,value2,value3, value4)\n",
    "            results.append(rows)\n",
    "            \n",
    "            finalBigrams = \"\"\n",
    "            \n",
    "            counter = counter + 1\n",
    "            \n",
    "            #gets scrubbed data from a given counter index\n",
    "            data = iterator(counter)\n",
    "            \n",
    "            #tokenizes data                                                \n",
    "            tokens = nltk.wordpunct_tokenize(str(data))\n",
    "            \n",
    "            nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "            \n",
    "            #this block removes junk characters and only accepts words 3 letters or longer\n",
    "            finder = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "            finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "            \n",
    "            #find top 200 best bigrams    \n",
    "            bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "            bigrams = list(sorted(finder.nbest(bigram_measures.likelihood_ratio, 20)))\n",
    "            \n",
    "            #convert bigrams to string for formatting\n",
    "            for x in bigrams: \n",
    "                finalBigrams += str(x)\n",
    "            \n",
    "\n",
    "            formatDataAndModel(finalBigrams, counter)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "    except:\n",
    "        print(\"error\")\n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
