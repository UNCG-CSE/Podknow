{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Much of code is credit to: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-7-d5b4781f8e75>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-d5b4781f8e75>\"\u001b[1;36m, line \u001b[1;32m37\u001b[0m\n\u001b[1;33m    C:\\Users\\Jeremy\\Downloads\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "def iterator(index):\n",
    "    \n",
    "    labels = ['ID','Name','Date','topicName','scrubbedtext']\n",
    "    podKnow_Data = pd.DataFrame.from_records(results, columns = labels)\n",
    "    \n",
    "    #isolate scrubbed text values and convert to lowercase to avoid duplicates\n",
    "    scrubbedData = str(podKnow_Data.iloc[index-1:index, 4].values).lower()\n",
    "    \n",
    "    #remove junk values\n",
    "    scrubbedData = scrubbedData.replace(\"\\\"\", \"\").replace(\",\", \"\").replace(\"\\'\",  \"\").splitlines()\n",
    "            \n",
    "    return scrubbedData\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "    \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "        \n",
    "def compute_coherence_values(self, dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \n",
    "    os.environ.update({'MALLET_HOME': r'C:/Users/Jeremy/Downloads/mallet'})\n",
    "    mallet_path = 'rC:\\\\Users\\\\Jeremy\\\\Downloads\\\\mallet\\\\bin\\\\mallet'\n",
    "    C:\\Users\\Jeremy\\Downloads\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "        \n",
    "\n",
    "results = []\n",
    "counter = 0\n",
    "totalList = []\n",
    "\n",
    "saveLocation = r'C:\\Users\\Jeremy\\Documents\\output\\scrubbed transcripts'\n",
    "for folderName,subfolders,fileName in os.walk(r'C:\\Users\\Jeremy\\Documents\\output\\scrubbed transcripts\\gcsst\\scrubbed'):\n",
    "    \n",
    "    for file in fileName:\n",
    "        if str(file.endswith(\".txt_scrubbed\")):\n",
    "            f = open(os.path.join(folderName,file),'rb')\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "            value0, value1, value2,value3, *extraWords = file.split('_')\n",
    "            value4 = data\n",
    "            rows = (value0,value1,value2,value3, value4)\n",
    "            results.append(rows)\n",
    "            \n",
    "            counter = counter + 1\n",
    "            \n",
    "            data = iterator(counter)\n",
    "            \n",
    "            data_words =  list(sent_to_words(data))\n",
    "\n",
    "            bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "            trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "        \n",
    "            bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "            trigram_mod = gensim.models.phrases.Phraser(trigram)  \n",
    "            \n",
    "            data_words_bigrams = make_bigrams(data_words)\n",
    "    \n",
    "            nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    \n",
    "            data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "            id2word = corpora.Dictionary(data_lemmatized)\n",
    "    \n",
    "            texts = data_lemmatized\n",
    "    \n",
    "            corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "            lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           alpha= 75,\n",
    "                                           eval_every=5,\n",
    "                                           per_word_topics = True,\n",
    "                                           passes=20)\n",
    "    \n",
    "            doc_lda = lda_model[corpus]\n",
    "                        \n",
    "            coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "            coherence_lda = coherence_model_lda.get_coherence()\n",
    "            \n",
    "            \n",
    "            model_list, coherence_values = compute_coherence_values(lda_model, dictionary=id2word, corpus=corpus, texts=texts, start=2, limit=40, step=6)\n",
    "            # Show graph\n",
    "            limit=40; start=2; step=6;\n",
    "            x = range(start, limit, step)\n",
    "            plt.plot(x, coherence_values)\n",
    "            plt.xlabel(\"Num Topics\")\n",
    "            plt.ylabel(\"Coherence score\")\n",
    "            plt.legend((\"coherence_values\"), loc='best')\n",
    "            \n",
    "            pngFileName = file.replace(\".txt_scrubbed\", \".png\")\n",
    "            plt.savefig(pngFileName)\n",
    "            \n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            \n",
    "              \n",
    "            lda_display = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, sort_topics=False)\n",
    "            pyLDAvis.enable_notebook()\n",
    "            \n",
    "            htmlFileName = file.replace(\".txt_scrubbed\", \".html\")\n",
    "            pyLDAvis.save_html(lda_display, htmlFileName)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "lsa_display = pyLDAvis.gensim.prepare(lsa_model, corpus, id2word)\n",
    "pyLDAvis.display(lsa_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
