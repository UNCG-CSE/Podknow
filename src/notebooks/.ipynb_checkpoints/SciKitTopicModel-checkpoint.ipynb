{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy\n",
    "from scipy.stats import expon\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import pearsonr\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1012570406', 'TheMFCEOProject', 'September112019', 'OvercomingTheFearsThatKeepUsOrdinarywithAndyFrisellaMFCEO314.txt', 'scrubbed']\n",
      "['1028908750', 'HiddenBrain', 'September162019', 'WereAllGonnaDie.txt', 'scrubbed']\n",
      "['1042368254', 'TheHerdwithColinCowherd', 'September142019', 'Blazing5Week2.txt', 'scrubbed']\n",
      "['1047335260', 'TheBenShapiroShow', 'September132019', 'Ep860TheFirstREALFightNightWarrensBlownOpportunity.txt', 'scrubbed']\n",
      "['1051557000', 'ScienceVs', 'August082019', 'TheDinosaurExplosion.txt', 'scrubbed']\n",
      "['1057255460', 'TheNPRPoliticsPodcast', 'September132019', 'The3rdDemocraticDebateTakeaways.txt', 'scrubbed']\n",
      "['1062418176', 'TRUECRIMEGARAGE', 'September112019', 'BrandonSwansonPart3334.txt', 'scrubbed']\n",
      "['1068563276', 'GettingCuriouswithJonathanVanNess', 'September112019', 'HowManyCatsAmIAllowedToFosterBeforeItsIllegalwithHannahShaw.txt', 'scrubbed']\n",
      "['1070322219', 'JockoPodcast', 'September112019', '194CostlyPitfallstoAvoidandCriticalManeuversHowtoWininAmericanBusinessWithPeteRoberts.txt', 'scrubbed']\n",
      "['1081124929', 'TheVanishedPodcast', 'September162019', 'JohnBrewer.txt', 'scrubbed']\n",
      "['1087110764', 'TheWayIHearditwithMikeRowe', 'September032019', 'Episode136ABoyAndHisDog.txt', 'scrubbed']\n",
      "['1089022756', 'PardonMyTake', 'September102019', 'RobGronkowski.txt', 'scrubbed']\n",
      "['1089216339', 'SomeoneKnowsSomething', 'July92019', 'IntroducingUncoverTheCatLadyCase.txt', 'scrubbed']\n",
      "['1091709555', 'Embedded', 'September52019', 'EndOfSummerUpdate.txt', 'scrubbed']\n",
      "['1112190608', 'CodeSwitch', 'September112019', 'ATaleOfTwoSchoolDistricts.txt', 'scrubbed']\n",
      "['1119389968', 'RevisionistHistory', 'August292019', 'TheQueenofCuba.txt', 'scrubbed']\n",
      "['1122804248', 'UnsolvedMurdersTrueCrimeStories', 'September102019', 'E163TheSaintValentinesDayMassacre.txt', 'scrubbed']\n",
      "['1126119288', 'TerribleThanksFOrAsking', 'September102019', 'LoveBabs.txt', 'scrubbed']\n",
      "['1140596919', 'UpandVanished', 'August12019', 'IntroducingMonsterPresentsInsomniac.txt', 'scrubbed']\n",
      "['1148175292', 'IntheDark', 'September102019', 'S2DonorEpisodePreview.txt', 'scrubbed']\n",
      "['1150088852', 'SkipandShannonUndisputed', 'September142019', 'WeeksBestJeffFisherErnestineSclafaniBaylessOBJDakPrescott.txt', 'scrubbed']\n",
      "['1150124880', 'SomethingYouShouldKnow', 'September142019', 'SYSKChoiceGreatLifeHacksTheNastyPoliticalDiscussionFix.txt', 'scrubbed']\n",
      "['1150510297', 'HowIBuiltThiswithGuyRaz', 'September162019', 'StitchFixKatrinaLake.txt', 'scrubbed']\n",
      "['1170959623', 'Crimetown', 'September052019', 'BonusEpisodeTheStreetsDontLoveYouBack.txt', 'scrubbed']\n",
      "['1178704872', 'GoalDigger', 'September162019', 'AreYouIntelligentClickPlaytoFindOut.txt', 'scrubbed']\n",
      "['1181233130', 'EDMYLETTShow', 'September112019', 'TrueAmericanHerowRobertONeil.txt', 'scrubbed']\n",
      "['1184022695', 'JordanPetersonPodcast', 'September82019', 'BePreciseInYourSpeech.txt', 'scrubbed']\n",
      "['1190981360', 'ThisPastWeekendwTheoVon', 'September92019', 'BonusNuggetThisPastWeekend229.txt', 'scrubbed']\n",
      "['1192761536', 'PodSaveAmerica', 'September132019', 'JoeputyourrecordsonDebaterecapspecial.txt', 'scrubbed']\n",
      "['1200343264', 'ForePlay', 'September122019', 'JuliInksterforAmerica.txt', 'scrubbed']\n",
      "['1200361736', 'TheDaily', 'September182019', 'KeepingHarveyWeinsteinsSecretsPart1LisaBloom.txt', 'scrubbed']\n",
      "['1201940175', 'CongratulationswithChrisDElia', 'September92019', 'Episode137GoodfellasInRealLife.txt', 'scrubbed']\n",
      "['1205030005', 'SerialKillers', 'September102019', 'IntroducingHoroscopeToday.txt', 'scrubbed']\n",
      "['1212558767', 'STown', 'March282017', 'ChapterI.txt', 'scrubbed']\n",
      "['121493675', 'NPRNewsNow', 'September162019', 'NPRNews09-16-201910AMET-September162019-NPRNewsNow.txt', 'scrubbed']\n",
      "['121493804', 'WaitWait...Don', 'tTellMe!', 'September72019', 'MaryWilson.txt', 'scrubbed']\n",
      "['1222114325', 'UpFirst', 'September182019', 'WednesdaySeptember18th2019.txt', 'scrubbed']\n",
      "['1232009882', 'TrueCrimeObsessed', 'September102019', '100TheImposterFOROUR100thEPISODEWEREGIVINGOUR1stEPISODEAREDO.txt', 'scrubbed']\n",
      "['1232428553', 'TheBreakfastClub', 'September132019', 'HitmakaInterviewMalcomGladwellandMore.txt', 'scrubbed']\n",
      "['1236778275', 'VIEWsWithDavidDobrikandJasonNash', 'September122019', 'MakingAMillionDollarsInOneNight.txt', 'scrubbed']\n",
      "['1237931798', 'WhereShouldWeBeginwithEstherPerel', 'March82019', 'SeasonThreeTrailer.txt', 'scrubbed']\n",
      "['1240841298', 'EarHustle', 'September112019', 'CatchAKite4.txt', 'scrubbed']\n",
      "['1245763628', 'RISEpodcast', 'September102019', '113WhyIsItSoHardtoTalkAboutMoneywithMarieForleo.txt', 'scrubbed']\n",
      "['1264843400', 'OprahsSuperSoulConversations', 'September182019', 'MalcolmGladwellTalkingtoStrangers.txt', 'scrubbed']\n",
      "['1272970334', 'DirtyJohn', 'August302019', 'IntroducingJoeExoticOverMyDeadBodySeason2.txt', 'scrubbed']\n",
      "['1275172907', 'DISGRACELAND', 'July302019', 'AfterParty6.txt', 'scrubbed']\n",
      "['1278815517', 'OlogieswithAlieWard', 'September092019', 'PotterologuPart1WIZARDSCIENCEwithRebeccaLai.txt', 'scrubbed']\n",
      "['1293860614', 'Haunted Places', 'September122019', 'ChaseVault.txt', 'scrubbed']\n",
      "['1299915173', 'THISPODCASRwillkillYOU', 'September022019', 'LymeDiseaseIdliketocheckouforticks.txt', 'scrubbed']\n",
      "['1305546061', 'CoffeeConvosPodcastwithKailLowryLindsieChrisley', 'September122019', 'EP96LindsiesTruthKailattheVMAsJavisBreakup.txt', 'scrubbed']\n",
      "['1313596069', 'AmericanHistoryTellers', 'September112019', 'DutchManhattanBuyingManhattan2.txt', 'scrubbed']\n",
      "['1317493077', 'DuolingoSpanishPodcast', 'September122019', 'LareinadelpelobuenoTheQueenofGoodHair.txt', 'scrubbed']\n",
      "['1322200189', 'CrimeJunkie', 'September162019', 'INFAMOUSDarlieRoutierPart2.txt', 'scrubbed']\n",
      "['1324249769', 'AtlantaMonster', 'August152019', 'IntroducingHitMan.txt', 'scrubbed']\n",
      "['1333316223', 'SmallDoseswithAmandaSeales', 'September122019', 'SideEffectsOfMoney.txt', 'scrubbed']\n",
      "['1334878780', 'TheDailyShowWithTrevorNoahEarsEdition.txt', 'scrubbed']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-41ec734e38b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalueList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mvalue4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalueList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalueList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalueList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalueList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for folderName,subfolders,fileName in os.walk('C:\\\\Users\\\\vince\\\\Desktop\\\\DataScience Project repo\\\\data\\\\transcripts\\\\gcsst\\\\scrubbed'):\n",
    "    for file in fileName:\n",
    "        if  str(file.endswith(\".txt_scrubbed\")):\n",
    "            f = open(os.path.join(folderName,file),'rb')\n",
    "            data = pickle.load(f)\n",
    "            valueList= file.split('_')\n",
    "            print(valueList)\n",
    "            value4 = data\n",
    "            if(len(valueList)==4):\n",
    "                rows = (valueList[0],valueList[1],valueList[2],valueList[3], value4)\n",
    "                results.append(rows)\n",
    "            else:\n",
    "                rows = (valueList[0], \"Null Name\", \"Null Date\", valueList[1], value4)\n",
    "            \n",
    "# labels = ['ID','Name','Date','topicName','scrubbedtext']\n",
    "# PodKnow_Data = pd.DataFrame.from_records(results, columns = labels)\n",
    "\n",
    "\n",
    "# data = PodKnow_Data.scrubbedtext.values.tolist()\n",
    "# # Used the below pprint line to check data collection\n",
    "# # pprint(data[:1])\n",
    "\n",
    "# def sent_to_words(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))\n",
    "        \n",
    "# data_words = list(sent_to_words(data))\n",
    "\n",
    "# # print(data_words[:1])\n",
    "\n",
    "# def lemmatization(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent))\n",
    "#         texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "#     return texts_out\n",
    "\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# data_lemmatized = lemmatization(data_words, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# # print(data_lemmatized[:1])\n",
    "\n",
    "# vectorizer = CountVectorizer(analyzer = 'word', min_df = 10, stop_words = 'english', lowercase = True, token_pattern='[a-zA-Z0-9]{3,}', max_features = 50000)\n",
    "\n",
    "# data_vectorized = data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "\n",
    "# # print(data_vectorized)\n",
    "\n",
    "# data_dense = data_vectorized.todense()\n",
    "\n",
    "# # Compute Sparsicity = Percentage of Non-Zero cells\n",
    "# print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "# lda_model = LatentDirichletAllocation(n_components=10, max_iter = 10, learning_method = 'online', random_state = 100, batch_size = 128, evaluate_every = -1, n_jobs = -1)\n",
    "\n",
    "# lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "# # print(lda_model)\n",
    "\n",
    "# # Log Likelyhood: Higher the better\n",
    "# print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "# print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# # See model parameters\n",
    "# pprint(lda_model.get_params())\n",
    "\n",
    "# # Define Search Param\n",
    "# search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# # Init the Model\n",
    "# lda = LatentDirichletAllocation()\n",
    "\n",
    "# # Init Grid Search Class\n",
    "# model = GridSearchCV(lda, param_grid=search_params, refit = True)\n",
    "\n",
    "# # Do the Grid Search, Multiple LDA models created very time consuming\n",
    "# model.fit(data_vectorized)\n",
    "\n",
    "# # Best Model\n",
    "# best_lda_model = model.best_estimator_\n",
    "\n",
    "# # Model Parameters\n",
    "# print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# # Log Likelihood Score\n",
    "# print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# # Perplexity\n",
    "# print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# # Create Document - Topic Matrix\n",
    "# lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# # column names\n",
    "# topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# # index names\n",
    "# docnames = [\"Podcast\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# # Make the pandas dataframe\n",
    "# df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# # Get dominant topic for each document\n",
    "# dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "# df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# # Styling\n",
    "# def color_green(val):\n",
    "#     color = 'green' if val > .1 else 'black'\n",
    "#     return 'color: {col}'.format(col=color)\n",
    "\n",
    "# def make_bold(val):\n",
    "#     weight = 700 if val > .1 else 400\n",
    "#     return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# # Apply Style\n",
    "# df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "# df_document_topics\n",
    "\n",
    "# # Topic distribution across all podcasts\n",
    "# df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "# df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "# df_topic_distribution\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "# panel\n",
    "\n",
    "\n",
    "# # Topic-Keyword Matrix\n",
    "# df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# # Assign Column and Index\n",
    "# df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "# df_topic_keywords.index = topicnames\n",
    "\n",
    "# # View\n",
    "# df_topic_keywords.head()\n",
    "\n",
    "\n",
    "# # Show top n keywords for each topic\n",
    "# def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "#     keywords = np.array(vectorizer.get_feature_names())\n",
    "#     topic_keywords = []\n",
    "#     for topic_weights in lda_model.components_:\n",
    "#         top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "#         topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "#     return topic_keywords\n",
    "\n",
    "# topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# # Topic - Keywords Dataframe\n",
    "# df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "# df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "# df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "# df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_topic_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7966951a52e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_topic_keywords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_topic_keywords' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
