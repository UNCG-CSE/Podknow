{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy\n",
    "from scipy.stats import expon\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import pearsonr\n",
    "import pylab as pl\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsicity:  22.150900327854554 %\n",
      "Log Likelihood:  -2057329.6937701486\n",
      "Perplexity:  965.1996740919108\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'online',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 10,\n",
      " 'n_jobs': -1,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 100,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -717384.4886271987\n",
      "Model Perplexity:  907.0229659577548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Topic 0</td>\n",
       "      <td>know</td>\n",
       "      <td>say</td>\n",
       "      <td>think</td>\n",
       "      <td>want</td>\n",
       "      <td>really</td>\n",
       "      <td>right</td>\n",
       "      <td>thing</td>\n",
       "      <td>guy</td>\n",
       "      <td>time</td>\n",
       "      <td>make</td>\n",
       "      <td>people</td>\n",
       "      <td>come</td>\n",
       "      <td>good</td>\n",
       "      <td>look</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 1</td>\n",
       "      <td>say</td>\n",
       "      <td>want</td>\n",
       "      <td>make</td>\n",
       "      <td>people</td>\n",
       "      <td>come</td>\n",
       "      <td>year</td>\n",
       "      <td>know</td>\n",
       "      <td>work</td>\n",
       "      <td>time</td>\n",
       "      <td>think</td>\n",
       "      <td>story</td>\n",
       "      <td>music</td>\n",
       "      <td>black</td>\n",
       "      <td>look</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 2</td>\n",
       "      <td>hair</td>\n",
       "      <td>police</td>\n",
       "      <td>mark</td>\n",
       "      <td>officer</td>\n",
       "      <td>black</td>\n",
       "      <td>man</td>\n",
       "      <td>year</td>\n",
       "      <td>woman</td>\n",
       "      <td>make</td>\n",
       "      <td>apartment</td>\n",
       "      <td>day</td>\n",
       "      <td>life</td>\n",
       "      <td>member</td>\n",
       "      <td>drug</td>\n",
       "      <td>feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 3</td>\n",
       "      <td>say</td>\n",
       "      <td>come</td>\n",
       "      <td>want</td>\n",
       "      <td>start</td>\n",
       "      <td>know</td>\n",
       "      <td>people</td>\n",
       "      <td>key</td>\n",
       "      <td>make</td>\n",
       "      <td>church</td>\n",
       "      <td>tell</td>\n",
       "      <td>year</td>\n",
       "      <td>story</td>\n",
       "      <td>school</td>\n",
       "      <td>right</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 4</td>\n",
       "      <td>people</td>\n",
       "      <td>think</td>\n",
       "      <td>say</td>\n",
       "      <td>know</td>\n",
       "      <td>thing</td>\n",
       "      <td>way</td>\n",
       "      <td>make</td>\n",
       "      <td>time</td>\n",
       "      <td>right</td>\n",
       "      <td>come</td>\n",
       "      <td>want</td>\n",
       "      <td>look</td>\n",
       "      <td>talk</td>\n",
       "      <td>world</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 5</td>\n",
       "      <td>know</td>\n",
       "      <td>people</td>\n",
       "      <td>think</td>\n",
       "      <td>really</td>\n",
       "      <td>thing</td>\n",
       "      <td>say</td>\n",
       "      <td>want</td>\n",
       "      <td>make</td>\n",
       "      <td>right</td>\n",
       "      <td>kind</td>\n",
       "      <td>way</td>\n",
       "      <td>time</td>\n",
       "      <td>work</td>\n",
       "      <td>good</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 6</td>\n",
       "      <td>know</td>\n",
       "      <td>really</td>\n",
       "      <td>people</td>\n",
       "      <td>disease</td>\n",
       "      <td>think</td>\n",
       "      <td>thing</td>\n",
       "      <td>make</td>\n",
       "      <td>lot</td>\n",
       "      <td>right</td>\n",
       "      <td>time</td>\n",
       "      <td>mean</td>\n",
       "      <td>good</td>\n",
       "      <td>tick</td>\n",
       "      <td>look</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 7</td>\n",
       "      <td>say</td>\n",
       "      <td>tell</td>\n",
       "      <td>know</td>\n",
       "      <td>year</td>\n",
       "      <td>time</td>\n",
       "      <td>case</td>\n",
       "      <td>make</td>\n",
       "      <td>come</td>\n",
       "      <td>help</td>\n",
       "      <td>murder</td>\n",
       "      <td>story</td>\n",
       "      <td>day</td>\n",
       "      <td>home</td>\n",
       "      <td>police</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 8</td>\n",
       "      <td>night</td>\n",
       "      <td>man</td>\n",
       "      <td>team</td>\n",
       "      <td>today</td>\n",
       "      <td>saint</td>\n",
       "      <td>story</td>\n",
       "      <td>year</td>\n",
       "      <td>lead</td>\n",
       "      <td>sport</td>\n",
       "      <td>death</td>\n",
       "      <td>know</td>\n",
       "      <td>episode</td>\n",
       "      <td>podcast</td>\n",
       "      <td>time</td>\n",
       "      <td>fan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic 9</td>\n",
       "      <td>people</td>\n",
       "      <td>think</td>\n",
       "      <td>know</td>\n",
       "      <td>prison</td>\n",
       "      <td>community</td>\n",
       "      <td>parent</td>\n",
       "      <td>want</td>\n",
       "      <td>young</td>\n",
       "      <td>life</td>\n",
       "      <td>make</td>\n",
       "      <td>really</td>\n",
       "      <td>thing</td>\n",
       "      <td>lot</td>\n",
       "      <td>talk</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 0  Word 1  Word 2   Word 3     Word 4  Word 5 Word 6 Word 7  \\\n",
       "Topic 0    know     say   think     want     really   right  thing    guy   \n",
       "Topic 1     say    want    make   people       come    year   know   work   \n",
       "Topic 2    hair  police    mark  officer      black     man   year  woman   \n",
       "Topic 3     say    come    want    start       know  people    key   make   \n",
       "Topic 4  people   think     say     know      thing     way   make   time   \n",
       "Topic 5    know  people   think   really      thing     say   want   make   \n",
       "Topic 6    know  really  people  disease      think   thing   make    lot   \n",
       "Topic 7     say    tell    know     year       time    case   make   come   \n",
       "Topic 8   night     man    team    today      saint   story   year   lead   \n",
       "Topic 9  people   think    know   prison  community  parent   want  young   \n",
       "\n",
       "         Word 8     Word 9 Word 10  Word 11  Word 12 Word 13 Word 14  \n",
       "Topic 0    time       make  people     come     good    look  little  \n",
       "Topic 1    time      think   story    music    black    look     man  \n",
       "Topic 2    make  apartment     day     life   member    drug    feel  \n",
       "Topic 3  church       tell    year    story   school   right  office  \n",
       "Topic 4   right       come    want     look     talk   world  really  \n",
       "Topic 5   right       kind     way     time     work    good    come  \n",
       "Topic 6   right       time    mean     good     tick    look    work  \n",
       "Topic 7    help     murder   story      day     home  police  family  \n",
       "Topic 8   sport      death    know  episode  podcast    time     fan  \n",
       "Topic 9    life       make  really    thing      lot    talk    love  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for folderName,subfolders,fileName in os.walk('C:\\\\Users\\\\Steve\\\\Desktop\\\\Podknow-master\\\\data\\\\transcripts\\\\gcsst\\\\scrubbed'):\n",
    "    for file in fileName:\n",
    "        if  str(file.endswith(\".txt_scrubbed\")):\n",
    "            f = open(os.path.join(folderName,file),'rb')\n",
    "            data = pickle.load(f)\n",
    "            value0, value1, value2, *extraWords = file.split('_')\n",
    "            value4 = data\n",
    "            rows = (value0,value1,value2, value3, value4)\n",
    "            results.append(rows)\n",
    "            \n",
    "            \n",
    "labels = ['ID','Name','Date','topicName','scrubbedtext']\n",
    "PodKnow_Data = pd.DataFrame.from_records(results, columns = labels)\n",
    "\n",
    "\n",
    "data = PodKnow_Data.scrubbedtext.values.tolist()\n",
    "# Used the below pprint line to check data collection\n",
    "# pprint(data[:1])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))\n",
    "        \n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# print(data_words[:1])\n",
    "\n",
    "def lemmatization(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "    return texts_out\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', min_df = 10, stop_words = 'english', lowercase = True, token_pattern='[a-zA-Z0-9]{3,}', max_features = 50000)\n",
    "\n",
    "data_vectorized = data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
    "\n",
    "# print(data_vectorized)\n",
    "\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, max_iter = 10, learning_method = 'online', random_state = 100, batch_size = 128, evaluate_every = -1, n_jobs = -1)\n",
    "\n",
    "lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "# print(lda_model)\n",
    "\n",
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See model parameters\n",
    "pprint(lda_model.get_params())\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params, refit = True)\n",
    "\n",
    "# Do the Grid Search, Multiple LDA models created very time consuming\n",
    "model.fit(data_vectorized)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# Create Document - Topic Matrix\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Podcast\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics\n",
    "\n",
    "# Topic distribution across all podcasts\n",
    "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "df_topic_distribution\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel\n",
    "\n",
    "\n",
    "# Topic-Keyword Matrix\n",
    "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
    "\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "\n",
    "# View\n",
    "df_topic_keywords.head()\n",
    "\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
